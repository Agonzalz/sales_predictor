{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset used :\n",
    "- https://www.kaggle.com/datasets/asaniczka/video-game-sales-2024\n",
    "\n",
    "## References\n",
    "- https://www.kaggle.com/code/jruots/forecasting-video-game-sales\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"vgchartz-2024.csv\")\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust output format\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "print(pd.value_counts(data[\"console\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data points for any outliers between highly correleated features. In this case critic score and total sales\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = data[\"critic_score\"], y = data[\"total_sales\"])\n",
    "plt.ylabel(\"total_sales\", fontsize=13)\n",
    "plt.xlabel(\"critic_score\", fontsize=13)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the outliers. Filtering process is up to our discretion.\n",
    "data = data.drop(data[(data[\"critic_score\"] > 6) & (data[\"total_sales\"] > 10)].index)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = data[\"critic_score\"], y = data[\"total_sales\"])\n",
    "plt.ylabel(\"total_sales\", fontsize=13)\n",
    "plt.xlabel(\"critic_score\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since total_sales is our target, we have to get rid of all rows with missing values for total_sales\n",
    "data = data.dropna(subset=[\"total_sales\"])\n",
    "\n",
    "#fill in missing critic score values with the mean of critic scores for each genre\n",
    "data[\"critic_score\"]  = data.groupby(\"genre\")['critic_score'].transform(\n",
    "\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "\n",
    "#Group publishers into \"Top 10\" and \"Other\" since there is too many unique publishers\n",
    "top_publishers = data['publisher'].value_counts().head(10).index\n",
    "data['publisher_grouped'] = data['publisher'].apply(\n",
    "\n",
    "    lambda x: x if x in top_publishers else 'Other'\n",
    "\n",
    ")\n",
    "\n",
    "#convert the release date to date_time and extract the month and date from it and put it into separate columns\n",
    "def parse_date(date):\n",
    "    try:\n",
    "        return pd.to_datetime(date, format=\"%d%b%y\")\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(date, format=\"%Y-%m-%d\")\n",
    "\n",
    "data[\"release_date\"] = data[\"release_date\"].apply(parse_date)\n",
    "data[\"month\"] = data[\"release_date\"].dt.month\n",
    "data[\"year\"] = data[\"release_date\"].dt.year\n",
    "\n",
    "# Drop irrelevant columns\n",
    "columns_to_drop = ['img', 'last_update', 'publisher', 'release_date', 'developer']\n",
    "\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode categorical features and log-transform sales (still not sure if log transform is necessary)\n",
    "data['Log_Total_Sales'] = np.log1p(data['total_sales'])\n",
    "\n",
    "encoded_data = pd.get_dummies(data, columns=['console', 'genre', 'publisher_grouped'], drop_first=True)\n",
    "\n",
    "X = encoded_data.drop(columns=['total_sales', 'Log_Total_Sales', 'title', 'na_sales', 'jp_sales', 'pal_sales', 'other_sales'])\n",
    "\n",
    "y = encoded_data['Log_Total_Sales']\n",
    "\n",
    "# Handle missing Year and Month values\n",
    "X['year'] = X['year'].fillna(X['year'].median())\n",
    "\n",
    "X['month'] = X['month'].fillna(X['month'].mode()[0])\n",
    "\n",
    "#split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "}\n",
    "\n",
    "#linear model fit and training\n",
    "models[\"Linear Regression\"].fit(X_train,y_train)\n",
    "y_pred = models[\"Linear Regression\"].predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R²: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save the model\n",
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(models[\"Linear Regression\"], \"linear_model.joblib\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load(\"linear_model.joblib\")\n",
    "\n",
    "# Use the loaded model for predictions\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "\n",
    "print(y_pred_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still need to work on this part more \n",
    "\n",
    "# Example game data (replace with actual game details)\n",
    "new_game = {\n",
    "    \"console\": \"PC\",\n",
    "    \"genre\": \"Action\",\n",
    "    \"Year\": 2021,\n",
    "    \"Month\": 11,\n",
    "    \"publisher_grouped\": \"Activision\",\n",
    "    \"critic_score\": 8.0\n",
    "}\n",
    "# Convert the input into a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "new_game_df = pd.DataFrame([new_game])\n",
    "\n",
    "# One-hot encode categorical features\n",
    "# Ensure one-hot encoding matches the training set\n",
    "new_game_encoded = pd.get_dummies(new_game_df, columns=[\"console\", \"genre\", \"publisher_grouped\"], drop_first=True)\n",
    "\n",
    "# Align columns with the training data (X_train)\n",
    "for col in X_train.columns:\n",
    "    if col not in new_game_encoded:\n",
    "        new_game_encoded[col] = 0  # Add missing columns with default values\n",
    "\n",
    "# Reorder columns to match the training data\n",
    "new_game_encoded = new_game_encoded[X_train.columns]\n",
    "# Use the trained model to predict\n",
    "predicted_log_sales = loaded_model.predict(new_game_encoded)\n",
    "\n",
    "# Convert the log-transformed prediction back to the original scale\n",
    "predicted_sales = np.expm1(predicted_log_sales)\n",
    "\n",
    "print(f\"Predicted Total Sales: {predicted_sales[0]:.2f} million units\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this module for hyperparameter tuning on the Random Forest and Gradient boost models\n",
    "#not sure if necessary. need to play around with it more. \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Hyperparameter grids\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"subsample\": [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Results dictionary to store the best models and their performance\n",
    "tuned_models = {}\n",
    "results = []\n",
    "\n",
    "for name, params in param_grids.items():\n",
    "    if name == \"Random Forest\":\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "    elif name == \"XGBoost\":\n",
    "        model = XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Use RandomizedSearchCV for faster tuning\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=params,\n",
    "        n_iter=20,  # Number of random combinations to try\n",
    "        scoring=\"neg_mean_absolute_error\",  # Use MAE for scoring\n",
    "        cv=3,  # 3-fold cross-validation\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Fit the search\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best model and its performance\n",
    "    tuned_models[name] = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "    best_score = -search.best_score_  # Convert negative MAE to positive\n",
    "    \n",
    "    results.append({\"Model\": name, \"Best MAE\": best_score, \"Best Params\": best_params})\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the best models\n",
    "final_results = []\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    # Fit the model with the best parameters\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    final_results.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R²\": r2})\n",
    "\n",
    "# Display the final results\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot for the models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Refined Scatter Plot\n",
    "for name, model in models.items():\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, label=\"Predictions\", color=\"blue\")\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\"red\", linestyle=\"--\", label=\"Perfect Prediction\")\n",
    "    plt.title(f\"{name}: Predicted vs. Actual\", fontsize=14)\n",
    "    plt.xlabel(\"Actual Log Total Sales\", fontsize=12)\n",
    "    plt.ylabel(\"Predicted Log Total Sales\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # Add R² score as annotation\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    plt.text(y_test.min(), y_test.max() - 0.5, f\"R² = {r2:.2f}\", fontsize=12, color=\"green\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined Line Plot for the models. Need to work on this plot more.\n",
    "sorted_indices = np.argsort(y_test.values)  # Sort by actual values for cleaner plotting\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Line plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test.values[sorted_indices], label=\"Actual\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "    plt.plot(y_pred[sorted_indices], label=\"Predicted\", marker=\"x\", linestyle=\"--\", color=\"orange\")\n",
    "    plt.title(f\"{name}: Actual vs. Predicted (Sorted)\", fontsize=14)\n",
    "    plt.xlabel(\"Sample Index (Sorted by Actual)\", fontsize=12)\n",
    "    plt.ylabel(\"Log Total Sales\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plot for models\n",
    "for name, model in models.items():\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Scatter residual plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.6, color=\"purple\")\n",
    "    plt.axhline(0, color=\"red\", linestyle=\"--\", label=\"Zero Error\")\n",
    "    plt.title(f\"{name}: Residuals Plot\", fontsize=14)\n",
    "    plt.xlabel(\"Predicted Log Total Sales\", fontsize=12)\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Histogram of residuals\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(residuals, bins=30, color=\"green\", alpha=0.7, edgecolor=\"black\")\n",
    "    plt.title(f\"{name}: Residuals Histogram\", fontsize=14)\n",
    "    plt.xlabel(\"Residuals\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything below is not necessary. It was my initial approach and is not being used. Still included for reference though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['console'] == 'PC') | (data['console'] == 'PS4') \n",
    "            | (data['console'] == 'NS') \n",
    "            | (data['console'] == 'XBL')| (data['console'] == 'PSN')\n",
    "            | (data['console'] == 'XOne')| (data['console'] == 'PS3')\n",
    "            | (data['console'] == 'X360')\n",
    "            | (data['console'] == 'Wii')]\n",
    "\n",
    "print(pd.value_counts(data[\"console\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_na = (data.isnull().sum() / len(data)) * 100\n",
    "data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "missing_data.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with missing target values\n",
    "data = data.dropna(subset=[\"total_sales\",\"critic_score\", \"release_date\", \"developer\"])\n",
    "print(data.shape)\n",
    "\n",
    "data_na = (data.isnull().sum() / len(data)) * 100\n",
    "data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "missing_data.head(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data[[\"console\", \"genre\", \"release_date\", \"publisher\", \"developer\", \"total_sales\", \"critic_score\"]]\n",
    "\n",
    "data_na = (data.isnull().sum() / len(data)) * 100\n",
    "data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "missing_data.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_publishers = data[\"publisher\"].value_counts().head(10).index\n",
    "top_developers = data[\"developer\"].value_counts().head(10).index\n",
    "\n",
    "\n",
    "data[\"publisher\"] = data[\"publisher\"].apply(lambda x: x if x in top_publishers else \"other_publishers\")\n",
    "data[\"developer\"] = data[\"developer\"].apply(lambda x: x if x in top_developers else \"other_developers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with missing categorical data\n",
    "data = data.dropna()\n",
    "\n",
    "def parse_date(date):\n",
    "    try:\n",
    "        return pd.to_datetime(date, format=\"%d%b%y\")\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(date, format=\"%Y-%m-%d\")\n",
    "\n",
    "data[\"release_date\"] = data[\"release_date\"].apply(parse_date)\n",
    "data[\"month\"] = data[\"release_date\"].dt.month\n",
    "data[\"year\"] = data[\"release_date\"].dt.year\n",
    "\n",
    "# Print first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data = data.drop(columns=[\"release_date\"])\n",
    "# Encode categorical variables\n",
    "data = pd.get_dummies(data, columns=[\"console\", \"genre\", \"developer\", \"publisher\"], drop_first=True)\n",
    "\n",
    "print(data.shape)\n",
    "# Print encoded data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = data.drop(columns=[\"total_sales\"])\n",
    "Y = data[\"total_sales\"]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"XTraining set size: {X_train.shape}\")\n",
    "print(f\"XTest set size: {X_test.shape}\")\n",
    "print(f\"YTraining set size: {Y_train.shape}\")\n",
    "print(f\"YTest set size: {Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publisher_mean_sales =  X_train.join(Y_train).groupby(\"publisher\")[\"total_sales\"].mean()\n",
    "\n",
    "# X_train[\"publisher_encoded\"] = X_train[\"publisher\"].map(publisher_mean_sales)\n",
    "# X_test[\"publisher_encoded\"] = X_test[\"publisher\"].map(publisher_mean_sales)\n",
    "\n",
    "# developer_mean_sales =  X_train.join(Y_train).groupby(\"developer\")[\"total_sales\"].mean()\n",
    "\n",
    "# X_train[\"developer_encoded\"] = X_train[\"developer\"].map(developer_mean_sales)\n",
    "# X_test[\"developer_encoded\"] = X_test[\"developer\"].map(developer_mean_sales)\n",
    "\n",
    "# # Handle missing encodings in test set\n",
    "# overall_mean = Y_train.mean()\n",
    "# X_test[\"publisher_encoded\"] = X_test[\"publisher_encoded\"].fillna(overall_mean)\n",
    "\n",
    "# overall_mean_developer = Y_train.mean()\n",
    "# X_test[\"developer_encoded\"] = X_test[\"developer_encoded\"].fillna(overall_mean_developer)\n",
    "\n",
    "# X_train = X_train.drop(columns=[\"publisher\"])\n",
    "# X_test = X_test.drop(columns=[\"publisher\"])\n",
    "\n",
    "\n",
    "# X_train = X_train.drop(columns=[\"developer\"])\n",
    "# X_test = X_test.drop(columns=[\"developer\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "sns.displot(data[\"total_sales\"], fit=norm);\n",
    "\n",
    "(mu, sigma) = norm.fit(data[\"total_sales\"])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('total_Sales distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(data['total_sales'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test data\n",
    "linear_predictions = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_linear = mean_absolute_error(Y_test, linear_predictions)\n",
    "rmse_linear = mean_squared_error(Y_test, linear_predictions, squared=False)\n",
    "r2_linear = r2_score(Y_test, linear_predictions)\n",
    "\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(f\"MAE: {mae_linear}\")\n",
    "print(f\"RMSE: {rmse_linear}\")\n",
    "print(f\"R² Score: {r2_linear}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "feature_importance_df =  pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(feature_importance_df[\"Feature\"].head(20), feature_importance_df[\"Importance\"].head(20))\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Important Features (Random Forest)\")\n",
    "plt.gca().invert_yaxis()  # Flip the y-axis for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importances = xgb_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print top 10 features\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df[\"Feature\"].head(10), feature_importance_df[\"Importance\"].head(10))\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Important Features (XGBoost)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(rf_model, X_test, Y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a DataFrame\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Importance\": perm_importance.importances_mean\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print top 10 features\n",
    "print(perm_importance_df.head(10))\n",
    "\n",
    "# Plot permutation importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(perm_importance_df[\"Feature\"].head(10), perm_importance_df[\"Importance\"].head(10))\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Features (Permutation Importance)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on test data\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_rf = mean_absolute_error(Y_test, rf_predictions)\n",
    "rmse_rf = mean_squared_error(Y_test, rf_predictions, squared=False)\n",
    "r2_rf = r2_score(Y_test, rf_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Performance:\")\n",
    "print(f\"MAE: {mae_rf}\")\n",
    "print(f\"RMSE: {rmse_rf}\")\n",
    "print(f\"R² Score: {r2_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.2, max_depth=10, random_state=42)\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test data\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_xgb = mean_absolute_error(Y_test, xgb_predictions)\n",
    "rmse_xgb = mean_squared_error(Y_test, xgb_predictions, squared=False)\n",
    "r2_xgb = r2_score(Y_test, xgb_predictions)\n",
    "\n",
    "print(\"XGBoost Performance:\")\n",
    "print(f\"MAE: {mae_xgb}\")\n",
    "print(f\"RMSE: {rmse_xgb}\")\n",
    "print(f\"R² Score: {r2_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics\n",
    "results = {\n",
    "    \"Model\": [\"Linear Regression\", \"Random Forest\", \"XGBoost\"],\n",
    "    \"MAE\": [mae_linear, mae_rf, mae_xgb],\n",
    "    \"RMSE\": [rmse_linear, rmse_rf, rmse_xgb],\n",
    "    \"R² Score\": [r2_linear, r2_rf, r2_xgb]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for actual vs. predicted sales (e.g., for Random Forest)\n",
    "plt.scatter(Y_test, rf_predictions, alpha=0.5)\n",
    "plt.xlabel(\"Actual Sales\")\n",
    "plt.ylabel(\"Predicted Sales\")\n",
    "plt.title(\"Actual vs. Predicted Sales (Random Forest)\")\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "feature_importances = rf_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Plot feature importance\n",
    "plt.barh(features, feature_importances)\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
